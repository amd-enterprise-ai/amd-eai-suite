diff --git a/workloads/llm-inference-vllm/helm/overrides/models/cohereforai_aya-expanse-32b-fp8.yaml b/workloads/llm-inference-vllm/helm/overrides/models/cohereforai_aya-expanse-32b-fp8.yaml
index 432e097..20a782b 100644
--- a/workloads/llm-inference-vllm/helm/overrides/models/cohereforai_aya-expanse-32b-fp8.yaml
+++ b/workloads/llm-inference-vllm/helm/overrides/models/cohereforai_aya-expanse-32b-fp8.yaml
@@ -8,8 +8,3 @@ served_model_name: "CohereForAI/aya-expanse-32b"
 
 vllm_engine_args:
   quantization: fp8
-
-env_vars:
-  HF_TOKEN:
-    key: hf-token
-    name: hf-token
diff --git a/workloads/llm-inference-vllm/helm/overrides/models/cohereforai_aya-expanse-32b.yaml b/workloads/llm-inference-vllm/helm/overrides/models/cohereforai_aya-expanse-32b.yaml
index 90c3ab4..e736022 100644
--- a/workloads/llm-inference-vllm/helm/overrides/models/cohereforai_aya-expanse-32b.yaml
+++ b/workloads/llm-inference-vllm/helm/overrides/models/cohereforai_aya-expanse-32b.yaml
@@ -6,7 +6,3 @@ metadata:
 model: "CohereForAI/aya-expanse-32b"
 served_model_name: "CohereForAI/aya-expanse-32b"
 
-env_vars:
-  HF_TOKEN:
-    key: hf-token
-    name: hf-token
diff --git a/workloads/llm-inference-vllm/helm/overrides/models/cohereforai_aya-expanse-8b.yaml b/workloads/llm-inference-vllm/helm/overrides/models/cohereforai_aya-expanse-8b.yaml
index d5e1b6b..4306079 100644
--- a/workloads/llm-inference-vllm/helm/overrides/models/cohereforai_aya-expanse-8b.yaml
+++ b/workloads/llm-inference-vllm/helm/overrides/models/cohereforai_aya-expanse-8b.yaml
@@ -5,8 +5,3 @@ metadata:
 
 model: "CohereForAI/aya-expanse-8b"
 served_model_name: "CohereForAI/aya-expanse-8b"
-
-env_vars:
-  HF_TOKEN:
-    key: hf-token
-    name: hf-token
diff --git a/workloads/llm-inference-vllm/helm/overrides/models/google_gemma-3-12b-it.yaml b/workloads/llm-inference-vllm/helm/overrides/models/google_gemma-3-12b-it.yaml
index a960acf..8521a8d 100644
--- a/workloads/llm-inference-vllm/helm/overrides/models/google_gemma-3-12b-it.yaml
+++ b/workloads/llm-inference-vllm/helm/overrides/models/google_gemma-3-12b-it.yaml
@@ -7,7 +7,4 @@ model: "google/gemma-3-12b-it"
 served_model_name: "google/gemma-3-12b-it"
 
 env_vars:
-  HF_TOKEN:
-    key: hf-token
-    name: hf-token
   VLLM_USE_V1: "1"
diff --git a/workloads/llm-inference-vllm/helm/overrides/models/google_gemma-3-1b-it.yaml b/workloads/llm-inference-vllm/helm/overrides/models/google_gemma-3-1b-it.yaml
index 97d691d..f1bc7d2 100644
--- a/workloads/llm-inference-vllm/helm/overrides/models/google_gemma-3-1b-it.yaml
+++ b/workloads/llm-inference-vllm/helm/overrides/models/google_gemma-3-1b-it.yaml
@@ -7,7 +7,4 @@ model: "google/gemma-3-1b-it"
 served_model_name: "google/gemma-3-1b-it"
 
 env_vars:
-  HF_TOKEN:
-    key: hf-token
-    name: hf-token
   VLLM_USE_V1: "1"
diff --git a/workloads/llm-inference-vllm/helm/overrides/models/google_gemma-3-27b-it.yaml b/workloads/llm-inference-vllm/helm/overrides/models/google_gemma-3-27b-it.yaml
index 9ab065f..fd77d73 100644
--- a/workloads/llm-inference-vllm/helm/overrides/models/google_gemma-3-27b-it.yaml
+++ b/workloads/llm-inference-vllm/helm/overrides/models/google_gemma-3-27b-it.yaml
@@ -7,7 +7,4 @@ model: "google/gemma-3-27b-it"
 served_model_name: "google/gemma-3-27b-it"
 
 env_vars:
-  HF_TOKEN:
-    key: hf-token
-    name: hf-token
   VLLM_USE_V1: "1"
diff --git a/workloads/llm-inference-vllm/helm/overrides/models/google_gemma-3-4b-it.yaml b/workloads/llm-inference-vllm/helm/overrides/models/google_gemma-3-4b-it.yaml
index f66f12b..4045fc7 100644
--- a/workloads/llm-inference-vllm/helm/overrides/models/google_gemma-3-4b-it.yaml
+++ b/workloads/llm-inference-vllm/helm/overrides/models/google_gemma-3-4b-it.yaml
@@ -7,7 +7,4 @@ model: "google/gemma-3-4b-it"
 served_model_name: "google/gemma-3-4b-it"
 
 env_vars:
-  HF_TOKEN:
-    key: hf-token
-    name: hf-token
   VLLM_USE_V1: "1"
diff --git a/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-3.1-405b-instruct.yaml b/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-3.1-405b-instruct.yaml
index 9d2f08c..469ed42 100644
--- a/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-3.1-405b-instruct.yaml
+++ b/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-3.1-405b-instruct.yaml
@@ -12,11 +12,6 @@ cpu_per_gpu: 4
 vllm_engine_args:
   gpu-memory-utilization: "0.95"
 
-env_vars:
-  HF_TOKEN:
-    key: hf-token
-    name: hf-token
-
 storage:
   ephemeral:
     quantity: 1024Gi
diff --git a/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-3.1-70b-instruct.yaml b/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-3.1-70b-instruct.yaml
index 38a96bc..d5dd2cd 100644
--- a/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-3.1-70b-instruct.yaml
+++ b/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-3.1-70b-instruct.yaml
@@ -10,11 +10,6 @@ vllm_engine_args:
   gpu-memory-utilization: "0.95"
   max-model-len: "65536"
 
-env_vars:
-  HF_TOKEN:
-    key: hf-token
-    name: hf-token
-
 storage:
   ephemeral:
     quantity: 256Gi
diff --git a/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-3.1-70b.yaml b/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-3.1-70b.yaml
index 5f6cb83..b57319f 100644
--- a/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-3.1-70b.yaml
+++ b/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-3.1-70b.yaml
@@ -10,11 +10,6 @@ vllm_engine_args:
   gpu-memory-utilization: "0.95"
   max-model-len: "65536"
 
-env_vars:
-  HF_TOKEN:
-    key: hf-token
-    name: hf-token
-
 storage:
   ephemeral:
     quantity: 256Gi
diff --git a/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-3.1-8b-instruct.yaml b/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-3.1-8b-instruct.yaml
index 3c2d280..161a4b9 100644
--- a/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-3.1-8b-instruct.yaml
+++ b/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-3.1-8b-instruct.yaml
@@ -5,8 +5,3 @@ metadata:
 
 model: "meta-llama/Llama-3.1-8B-Instruct"
 served_model_name: "meta-llama/Llama-3.1-8B-Instruct"
-
-env_vars:
-  HF_TOKEN:
-    key: hf-token
-    name: hf-token
diff --git a/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-3.1-8b.yaml b/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-3.1-8b.yaml
index 1d73df4..e4d1a7e 100644
--- a/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-3.1-8b.yaml
+++ b/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-3.1-8b.yaml
@@ -5,8 +5,3 @@ metadata:
 
 model: "meta-llama/Llama-3.1-8B"
 served_model_name: "meta-llama/Llama-3.1-8B"
-
-env_vars:
-  HF_TOKEN:
-    key: hf-token
-    name: hf-token
diff --git a/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-3.2-11b-vision-instruct.yaml b/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-3.2-11b-vision-instruct.yaml
index 0c35030..324171b 100644
--- a/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-3.2-11b-vision-instruct.yaml
+++ b/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-3.2-11b-vision-instruct.yaml
@@ -5,8 +5,3 @@ metadata:
 
 model: "meta-llama/Llama-3.2-11B-Vision-Instruct"
 served_model_name: "meta-llama/Llama-3.2-11B-Vision-Instruct"
-
-env_vars:
-  HF_TOKEN:
-    key: hf-token
-    name: hf-token
diff --git a/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-3.2-1b-instruct.yaml b/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-3.2-1b-instruct.yaml
index 75f5704..c102d15 100644
--- a/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-3.2-1b-instruct.yaml
+++ b/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-3.2-1b-instruct.yaml
@@ -5,8 +5,3 @@ metadata:
 
 model: "meta-llama/Llama-3.2-1B-Instruct"
 served_model_name: "meta-llama/Llama-3.2-1B-Instruct"
-
-env_vars:
-  HF_TOKEN:
-    key: hf-token
-    name: hf-token
diff --git a/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-3.2-3b-instruct.yaml b/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-3.2-3b-instruct.yaml
index b1bf652..944e73f 100644
--- a/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-3.2-3b-instruct.yaml
+++ b/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-3.2-3b-instruct.yaml
@@ -5,8 +5,3 @@ metadata:
 
 model: "meta-llama/Llama-3.2-3B-Instruct"
 served_model_name: "meta-llama/Llama-3.2-3B-Instruct"
-
-env_vars:
-  HF_TOKEN:
-    key: hf-token
-    name: hf-token
diff --git a/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-3.2-90b-vision-instruct.yaml b/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-3.2-90b-vision-instruct.yaml
index 9e553ad..3235ee9 100644
--- a/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-3.2-90b-vision-instruct.yaml
+++ b/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-3.2-90b-vision-instruct.yaml
@@ -8,8 +8,3 @@ served_model_name: "meta-llama/Llama-3.2-90B-Vision-Instruct"
 gpus: 4
 memory_per_gpu: 64 # Gi
 cpu_per_gpu: 4
-
-env_vars:
-  HF_TOKEN:
-    key: hf-token
-    name: hf-token
diff --git a/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-3.3-70b-instruct.yaml b/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-3.3-70b-instruct.yaml
index 40ee20e..071ef0b 100644
--- a/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-3.3-70b-instruct.yaml
+++ b/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-3.3-70b-instruct.yaml
@@ -10,11 +10,6 @@ vllm_engine_args:
   gpu-memory-utilization: "0.95"
   max-model-len: "65536"
 
-env_vars:
-  HF_TOKEN:
-    key: hf-token
-    name: hf-token
-
 storage:
   ephemeral:
     quantity: 256Gi
diff --git a/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-4-maverick-17b-128e-instruct.yaml b/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-4-maverick-17b-128e-instruct.yaml
index 4dfbb8b..55f9d17 100644
--- a/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-4-maverick-17b-128e-instruct.yaml
+++ b/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-4-maverick-17b-128e-instruct.yaml
@@ -22,9 +22,6 @@ env_vars:
   VLLM_WORKER_MULTIPROC_METHOD: "spawn"
   VLLM_USE_MODELSCOPE: "False"
   VLLM_USE_TRITON_FLASH_ATTN: "0"
-  HF_TOKEN:
-    key: hf-token
-    name: hf-token
 
 storage:
   ephemeral:
diff --git a/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-4-scout-17b-16e-instruct.yaml b/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-4-scout-17b-16e-instruct.yaml
index 1405eae..d5be03f 100644
--- a/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-4-scout-17b-16e-instruct.yaml
+++ b/workloads/llm-inference-vllm/helm/overrides/models/meta-llama_llama-4-scout-17b-16e-instruct.yaml
@@ -22,9 +22,6 @@ env_vars:
   VLLM_WORKER_MULTIPROC_METHOD: "spawn"
   VLLM_USE_MODELSCOPE: "False"
   VLLM_USE_TRITON_FLASH_ATTN: "0"
-  HF_TOKEN:
-    key: hf-token
-    name: hf-token
 
 storage:
   ephemeral:
diff --git a/workloads/llm-inference-vllm/helm/overrides/models/mistralai_mistral-large-instruct-2411.yaml b/workloads/llm-inference-vllm/helm/overrides/models/mistralai_mistral-large-instruct-2411.yaml
index 40025e0..5eb5ed9 100644
--- a/workloads/llm-inference-vllm/helm/overrides/models/mistralai_mistral-large-instruct-2411.yaml
+++ b/workloads/llm-inference-vllm/helm/overrides/models/mistralai_mistral-large-instruct-2411.yaml
@@ -7,11 +7,6 @@ model: "mistralai/Mistral-Large-Instruct-2411"
 served_model_name: "mistralai/Mistral-Large-Instruct-2411"
 gpus: 2
 
-env_vars:
-  HF_TOKEN:
-    key: hf-token
-    name: hf-token
-
 storage:
   ephemeral:
     quantity: 512Gi  # observed peak usage: ~457Gi (2 copies of the model)
diff --git a/workloads/llm-inference-vllm/helm/overrides/models/mistralai_mistral-small-3.1-24b-instruct-2503.yaml b/workloads/llm-inference-vllm/helm/overrides/models/mistralai_mistral-small-3.1-24b-instruct-2503.yaml
index b3f7f28..33ee49e 100644
--- a/workloads/llm-inference-vllm/helm/overrides/models/mistralai_mistral-small-3.1-24b-instruct-2503.yaml
+++ b/workloads/llm-inference-vllm/helm/overrides/models/mistralai_mistral-small-3.1-24b-instruct-2503.yaml
@@ -10,7 +10,4 @@ vllm_engine_args:
   max-model-len: "32768"
 
 env_vars:
-  HF_TOKEN:
-    key: hf-token
-    name: hf-token
   VLLM_USE_TRITON_FLASH_ATTN: "0"
diff --git a/workloads/llm-inference-vllm/helm/overrides/models/mistralai_mixtral-8x22b-instruct-v0.1.yaml b/workloads/llm-inference-vllm/helm/overrides/models/mistralai_mixtral-8x22b-instruct-v0.1.yaml
index 0bf7159..6bcdc8a 100644
--- a/workloads/llm-inference-vllm/helm/overrides/models/mistralai_mixtral-8x22b-instruct-v0.1.yaml
+++ b/workloads/llm-inference-vllm/helm/overrides/models/mistralai_mixtral-8x22b-instruct-v0.1.yaml
@@ -7,11 +7,6 @@ model: "mistralai/Mixtral-8x22B-Instruct-v0.1"
 served_model_name: "mistralai/Mixtral-8x22B-Instruct-v0.1"
 gpus: 4
 
-env_vars:
-  HF_TOKEN:
-    key: hf-token
-    name: hf-token
-
 storage:
   ephemeral:
     quantity: 300Gi
diff --git a/workloads/llm-inference-vllm/helm/overrides/models/mistralai_mixtral-8x7b-instruct-v0.1.yaml b/workloads/llm-inference-vllm/helm/overrides/models/mistralai_mixtral-8x7b-instruct-v0.1.yaml
index 7f18081..c1527f8 100644
--- a/workloads/llm-inference-vllm/helm/overrides/models/mistralai_mixtral-8x7b-instruct-v0.1.yaml
+++ b/workloads/llm-inference-vllm/helm/overrides/models/mistralai_mixtral-8x7b-instruct-v0.1.yaml
@@ -6,11 +6,6 @@ metadata:
 model: "mistralai/Mixtral-8x7B-Instruct-v0.1"
 served_model_name: "mistralai/Mixtral-8x7B-Instruct-v0.1"
 
-env_vars:
-  HF_TOKEN:
-    key: hf-token
-    name: hf-token
-
 storage:
   ephemeral:
     quantity: 256Gi
diff --git a/workloads/llm-inference-vllm/helm/overrides/models/odiagenai-llm_qwen-1.5-odia-7b.yaml b/workloads/llm-inference-vllm/helm/overrides/models/odiagenai-llm_qwen-1.5-odia-7b.yaml
index 3fd2037..a6a6066 100644
--- a/workloads/llm-inference-vllm/helm/overrides/models/odiagenai-llm_qwen-1.5-odia-7b.yaml
+++ b/workloads/llm-inference-vllm/helm/overrides/models/odiagenai-llm_qwen-1.5-odia-7b.yaml
@@ -5,8 +5,3 @@ metadata:
 
 model: "OdiaGenAI-LLM/qwen_1.5_odia_7b"
 served_model_name: "OdiaGenAI-LLM/qwen_1.5_odia_7b"
-
-env_vars:
-  HF_TOKEN:
-    key: hf-token
-    name: hf-token
diff --git a/workloads/llm-inference-vllm/helm/overrides/models/unsloth_llama-3.3-70b-instruct.yaml b/workloads/llm-inference-vllm/helm/overrides/models/unsloth_llama-3.3-70b-instruct.yaml
index ee13983..1144a06 100644
--- a/workloads/llm-inference-vllm/helm/overrides/models/unsloth_llama-3.3-70b-instruct.yaml
+++ b/workloads/llm-inference-vllm/helm/overrides/models/unsloth_llama-3.3-70b-instruct.yaml
@@ -10,11 +10,6 @@ vllm_engine_args:
   gpu-memory-utilization: "0.95"
   max-model-len: "65536"
 
-env_vars:
-  HF_TOKEN:
-    key: hf-token
-    name: hf-token
-
 storage:
   ephemeral:
     quantity: 256Gi
