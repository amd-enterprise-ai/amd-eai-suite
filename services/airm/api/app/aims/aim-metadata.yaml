# Copyright Â© Advanced Micro Devices, Inc., or its affiliates.
#
# SPDX-License-Identifier: MIT

generated_at: "2025-11-11T00:00:00.000000+00:00"
images:
  - name: aim-meta-llama-llama-3-1-8b-instruct
    tags:
      - image: docker.io/amdenterpriseai/aim-meta-llama-llama-3-1-8b-instruct:0.8.4
        labels:
          com.amd.aim.description.full:
            The Meta Llama 3.1 collection, released by Meta
            on July 23, 2024, is a set of multilingual large language models available
            in 8B, 70B, and 405B parameter sizes. The models are offered in both pretrained
            and instruction-tuned versions, with the latter being optimized for dialogue
            use cases. Built on an auto-regressive transformer architecture with Grouped-Query
            Attention (GQA), the tuned models are refined using Supervised Fine-Tuning
            (SFT) and Reinforcement Learning with Human Feedback (RLHF). Llama 3.1 is
            intended for commercial and research applications. The instruction-tuned models
            are designed for assistant-like chat, while the pretrained models can be adapted
            for various natural language generation tasks. The custom commercial license
            also allows for using the model's output for synthetic data generation and
            distillation to improve other models.
          com.amd.aim.hfToken.required: "True"
          com.amd.aim.model.canonicalName: meta-llama/Llama-3.1-8B-Instruct
          com.amd.aim.model.recommendedDeployments:
            "{'gpuModel': 'MI300X', 'gpuCount':
            1, 'precision': 'fp8', 'metric': 'latency', 'description': 'Optimized
            for latency on MI300X using fp8 precision'}, {'gpuModel': 'MI300X', 'gpuCount':
            1, 'precision': 'fp8', 'metric': 'throughput', 'description': 'Optimized
            for throughput on MI300X using fp8 precision'}"
          com.amd.aim.model.source: https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct
          com.amd.aim.model.tags: text-generation, chat, instruction
          com.amd.aim.model.variants: amd/Llama-3.1-8B-Instruct-FP8-KV, meta-llama/Llama-3.1-8B-Instruct
          com.amd.aim.release.notes: ""
          com.amd.aim.title: meta-llama/Llama-3.1-8B-Instruct
          org.opencontainers.image.authors: ""
          org.opencontainers.image.created: "2025-11-11T10:45:11Z"
          org.opencontainers.image.description:
            Instruction-tuned version of Llama 3.1
            8B optimized for chat and instruction following.
          org.opencontainers.image.documentation: ""
          org.opencontainers.image.licenses: llama3.1, MIT
          org.opencontainers.image.ref.name: ubuntu
          org.opencontainers.image.revision: bdba5c8c0a6be5962f098fa4bbe0ecbce9fdc07a
          org.opencontainers.image.source: https://github.com/silogen/aim-build
          org.opencontainers.image.title: meta-llama/Llama-3.1-8B-Instruct
          org.opencontainers.image.vendor: AMD
          org.opencontainers.image.version: ""
        tag: 0.8.4
  - name: aim-meta-llama-llama-3-3-70b-instruct
    tags:
      - image: docker.io/amdenterpriseai/aim-meta-llama-llama-3-3-70b-instruct:0.8.4-preview
        labels:
          com.amd.aim.description.full: ""
          com.amd.aim.hfToken.required: "True"
          com.amd.aim.model.canonicalName: meta-llama/Llama-3.3-70B-Instruct
          com.amd.aim.model.recommendedDeployments:
            "{'gpuModel': 'MI300X', 'gpuCount':
            1, 'precision': 'fp8', 'metric': 'latency', 'description': 'Optimized
            for latency on MI300X using fp8 precision'}, {'gpuModel': 'MI300X', 'gpuCount':
            1, 'precision': 'fp8', 'metric': 'throughput', 'description': 'Optimized
            for throughput on MI300X using fp8 precision'}"
          com.amd.aim.model.source: https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct
          com.amd.aim.model.tags: text-generation, chat, instruction
          com.amd.aim.model.variants: amd/Llama-3.3-70B-Instruct-FP8-KV, meta-llama/Llama-3.3-70B-Instruct
          com.amd.aim.release.notes: ""
          com.amd.aim.title: meta-llama/Llama-3.3-70B-Instruct
          org.opencontainers.image.authors: ""
          org.opencontainers.image.created: "2025-11-11T09:42:08Z"
          org.opencontainers.image.description:
            Latest instruction-tuned Llama 3.3 model
            with 70B parameters and improved performance.
          org.opencontainers.image.documentation: ""
          org.opencontainers.image.licenses: llama3.3, MIT
          org.opencontainers.image.ref.name: ubuntu
          org.opencontainers.image.revision: ee2284cf1d8481c5e9dee3192d97104a8507d062
          org.opencontainers.image.source: https://github.com/silogen/aim-build
          org.opencontainers.image.title: meta-llama/Llama-3.3-70B-Instruct
          org.opencontainers.image.vendor: AMD
          org.opencontainers.image.version: ""
        tag: 0.8.4-preview
  - name: aim-qwen-qwen3-32b
    tags:
      - image: docker.io/amdenterpriseai/aim-qwen-qwen3-32b:0.8.4
        labels:
          com.amd.aim.description.full: ""
          com.amd.aim.hfToken.required: "False"
          com.amd.aim.model.canonicalName: Qwen/Qwen3-32B
          com.amd.aim.model.recommendedDeployments:
            "{'gpuModel': 'MI300X', 'gpuCount':
            1, 'precision': 'fp16', 'metric': 'latency', 'description': 'Optimized
            for latency on MI300X using fp16 precision'}, {'gpuModel': 'MI300X',
            'gpuCount': 1, 'precision': 'fp16', 'metric': 'throughput', 'description':
            'Optimized for throughput on MI300X using fp16 precision'}"
          com.amd.aim.model.source: https://huggingface.co/Qwen/Qwen3-32B
          com.amd.aim.model.tags: text-generation, chat
          com.amd.aim.model.variants: Qwen/Qwen3-32B, Qwen/Qwen3-32B-FP8
          com.amd.aim.release.notes: ""
          com.amd.aim.title: Qwen/Qwen3-32B
          org.opencontainers.image.authors: ""
          org.opencontainers.image.created: "2025-11-11T10:45:11Z"
          org.opencontainers.image.description:
            Qwen is the large language model and large
            multimodal model series of the Qwen Team, Alibaba Group.
          org.opencontainers.image.documentation: ""
          org.opencontainers.image.licenses: Apache-2.0, MIT
          org.opencontainers.image.ref.name: ubuntu
          org.opencontainers.image.revision: bdba5c8c0a6be5962f098fa4bbe0ecbce9fdc07a
          org.opencontainers.image.source: https://github.com/silogen/aim-build
          org.opencontainers.image.title: Qwen/Qwen3-32B
          org.opencontainers.image.vendor: AMD
          org.opencontainers.image.version: ""
        tag: 0.8.4
  - name: aim-mistralai-mistral-small-3-2-24b-instruct-2506
    tags:
      - image: docker.io/amdenterpriseai/aim-mistralai-mistral-small-3-2-24b-instruct-2506:0.8.4
        labels:
          com.amd.aim.description.full: ""
          com.amd.aim.hfToken.required: "False"
          com.amd.aim.model.canonicalName: mistralai/Mistral-Small-3.2-24B-Instruct-2506
          com.amd.aim.model.recommendedDeployments:
            "{'gpuModel': 'MI300X', 'gpuCount':
            1, 'precision': 'fp16', 'metric': 'latency', 'description': 'Optimized
            for latency on MI300X using fp16 precision'}, {'gpuModel': 'MI300X',
            'gpuCount': 1, 'precision': 'fp16', 'metric': 'throughput', 'description':
            'Optimized for throughput on MI300X using fp16 precision'}"
          com.amd.aim.model.source: https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506
          com.amd.aim.model.tags: text-generation, chat
          com.amd.aim.model.variants: mistralai/Mistral-Small-3.2-24B-Instruct-2506
          com.amd.aim.release.notes: ""
          com.amd.aim.title: mistralai/Mistral-Small-3.2-24B-Instruct-2506
          org.opencontainers.image.authors: ""
          org.opencontainers.image.created: "2025-11-11T10:45:11Z"
          org.opencontainers.image.description:
            "Mistral-Small-3.2-24B-Instruct-2506 is
            a minor update of Mistral-Small-3.1-24B-Instruct-2503. Small-3.2 improves
            in the following categories: - Instruction following: Small-3.2 is better
            at following precise instructions - Repetition errors: Small-3.2 produces
            less infinite generations or repetitive answers - Function calling: Small-3.2's
            function calling template is more robust"
          org.opencontainers.image.documentation: ""
          org.opencontainers.image.licenses: Apache-2.0, MIT
          org.opencontainers.image.ref.name: ubuntu
          org.opencontainers.image.revision: bdba5c8c0a6be5962f098fa4bbe0ecbce9fdc07a
          org.opencontainers.image.source: https://github.com/silogen/aim-build
          org.opencontainers.image.title: mistralai/Mistral-Small-3.2-24B-Instruct-2506
          org.opencontainers.image.vendor: AMD
          org.opencontainers.image.version: ""
        tag: 0.8.4
  - name: aim-meta-llama-llama-3-1-405b-instruct
    tags:
      - image: docker.io/amdenterpriseai/aim-meta-llama-llama-3-1-405b-instruct:0.8.4
        labels:
          com.amd.aim.description.full: ""
          com.amd.aim.hfToken.required: "True"
          com.amd.aim.model.canonicalName: meta-llama/Llama-3.1-405B-Instruct
          com.amd.aim.model.recommendedDeployments:
            "{'gpuModel': 'MI300X', 'gpuCount':
            8, 'precision': 'fp8', 'metric': 'latency', 'description': 'Optimized
            for latency on MI300X using fp8 precision'}, {'gpuModel': 'MI300X', 'gpuCount':
            8, 'precision': 'fp8', 'metric': 'throughput', 'description': 'Optimized
            for throughput on MI300X using fp8 precision'}"
          com.amd.aim.model.source: https://huggingface.co/meta-llama/Llama-3.1-405B-Instruct
          com.amd.aim.model.tags: text-generation, chat, instruction
          com.amd.aim.model.variants: amd/Llama-3.1-405B-Instruct-FP8-KV, meta-llama/Llama-3.1-405B-Instruct
          com.amd.aim.release.notes: ""
          com.amd.aim.title: meta-llama/Llama-3.1-405B-Instruct
          org.opencontainers.image.authors: ""
          org.opencontainers.image.created: "2025-11-11T10:45:11Z"
          org.opencontainers.image.description:
            Massive instruction-tuned version of Llama
            3.1 with 405B parameters for the most demanding tasks.
          org.opencontainers.image.documentation: ""
          org.opencontainers.image.licenses: llama3.1, MIT
          org.opencontainers.image.ref.name: ubuntu
          org.opencontainers.image.revision: bdba5c8c0a6be5962f098fa4bbe0ecbce9fdc07a
          org.opencontainers.image.source: https://github.com/silogen/aim-build
          org.opencontainers.image.title: meta-llama/Llama-3.1-405B-Instruct
          org.opencontainers.image.vendor: AMD
          org.opencontainers.image.version: ""
        tag: 0.8.4
  - name: aim-mistralai-mixtral-8x22b-instruct-v0-1
    tags:
      - image: docker.io/amdenterpriseai/aim-mistralai-mixtral-8x22b-instruct-v0-1:0.8.4
        labels:
          com.amd.aim.description.full: ""
          com.amd.aim.hfToken.required: "False"
          com.amd.aim.model.canonicalName: mistralai/Mixtral-8x22B-Instruct-v0.1
          com.amd.aim.model.recommendedDeployments:
            "{'gpuModel': 'MI300X', 'gpuCount':
            8, 'precision': 'fp16', 'metric': 'latency', 'description': 'Optimized
            for latency on MI300X using fp16 precision'}, {'gpuModel': 'MI300X',
            'gpuCount': 8, 'precision': 'fp16', 'metric': 'throughput', 'description':
            'Optimized for throughput on MI300X using fp16 precision'}"
          com.amd.aim.model.source: https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1
          com.amd.aim.model.tags: text-generation, chat, instruction
          com.amd.aim.model.variants: mistralai/Mixtral-8x22B-Instruct-v0.1
          com.amd.aim.release.notes: ""
          com.amd.aim.title: mistralai/Mixtral-8x22B-Instruct-v0.1
          org.opencontainers.image.authors: ""
          org.opencontainers.image.created: "2025-11-11T10:45:11Z"
          org.opencontainers.image.description:
            Mixture of experts model with 8 experts
            of 22B parameters each for efficient scaling.
          org.opencontainers.image.documentation: ""
          org.opencontainers.image.licenses: Apache-2.0, MIT
          org.opencontainers.image.ref.name: ubuntu
          org.opencontainers.image.revision: bdba5c8c0a6be5962f098fa4bbe0ecbce9fdc07a
          org.opencontainers.image.source: https://github.com/silogen/aim-build
          org.opencontainers.image.title: mistralai/Mixtral-8x22B-Instruct-v0.1
          org.opencontainers.image.vendor: AMD
          org.opencontainers.image.version: ""
        tag: 0.8.4
  - name: aim-mistralai-mixtral-8x7b-instruct-v0-1
    tags:
      - image: docker.io/amdenterpriseai/aim-mistralai-mixtral-8x7b-instruct-v0-1:0.8.4
        labels:
          com.amd.aim.description.full: ""
          com.amd.aim.hfToken.required: "False"
          com.amd.aim.model.canonicalName: mistralai/Mixtral-8x7B-Instruct-v0.1
          com.amd.aim.model.recommendedDeployments:
            "{'gpuModel': 'MI300X', 'gpuCount':
            1, 'precision': 'fp16', 'metric': 'latency', 'description': 'Optimized
            for latency on MI300X using fp16 precision'}, {'gpuModel': 'MI300X',
            'gpuCount': 1, 'precision': 'fp16', 'metric': 'throughput', 'description':
            'Optimized for throughput on MI300X using fp16 precision'}"
          com.amd.aim.model.source: https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1
          com.amd.aim.model.tags: text-generation, chat
          com.amd.aim.model.variants: mistralai/Mixtral-8x7B-Instruct-v0.1
          com.amd.aim.release.notes: ""
          com.amd.aim.title: mistralai/Mixtral-8x7B-Instruct-v0.1
          org.opencontainers.image.authors: ""
          org.opencontainers.image.created: "2025-11-11T10:45:11Z"
          org.opencontainers.image.description:
            The Mixtral-8x7B Large Language Model
            (LLM) is a pretrained generative Sparse Mixture of Experts.
          org.opencontainers.image.documentation: ""
          org.opencontainers.image.licenses: Apache-2.0, MIT
          org.opencontainers.image.ref.name: ubuntu
          org.opencontainers.image.revision: bdba5c8c0a6be5962f098fa4bbe0ecbce9fdc07a
          org.opencontainers.image.source: https://github.com/silogen/aim-build
          org.opencontainers.image.title: mistralai/Mixtral-8x7B-Instruct-v0.1
          org.opencontainers.image.vendor: AMD
          org.opencontainers.image.version: ""
        tag: 0.8.4
  - name: aim-meta-llama-llama-3-2-1b-instruct
    tags:
      - image: docker.io/amdenterpriseai/aim-meta-llama-llama-3-2-1b-instruct:0.8.4
        labels:
          com.amd.aim.description.full: ""
          com.amd.aim.hfToken.required: "True"
          com.amd.aim.model.canonicalName: meta-llama/Llama-3.2-1B-Instruct
          com.amd.aim.model.recommendedDeployments:
            "{'gpuModel': 'MI300X', 'gpuCount':
            1, 'precision': 'fp16', 'metric': 'latency', 'description': 'Optimized
            for latency on MI300X using fp16 precision'}, {'gpuModel': 'MI300X',
            'gpuCount': 1, 'precision': 'fp16', 'metric': 'throughput', 'description':
            'Optimized for throughput on MI300X using fp16 precision'}"
          com.amd.aim.model.source: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct
          com.amd.aim.model.tags: text-generation, chat, instruction
          com.amd.aim.model.variants: meta-llama/Llama-3.2-1B-Instruct
          com.amd.aim.release.notes: ""
          com.amd.aim.title: meta-llama/Llama-3.2-1B-Instruct
          org.opencontainers.image.authors: ""
          org.opencontainers.image.created: "2025-11-11T10:45:11Z"
          org.opencontainers.image.description:
            Compact instruction-tuned Llama 3.2 model
            with 1B parameters for edge deployment.
          org.opencontainers.image.documentation: ""
          org.opencontainers.image.licenses: llama3.2, MIT
          org.opencontainers.image.ref.name: ubuntu
          org.opencontainers.image.revision: bdba5c8c0a6be5962f098fa4bbe0ecbce9fdc07a
          org.opencontainers.image.source: https://github.com/silogen/aim-build
          org.opencontainers.image.title: meta-llama/Llama-3.2-1B-Instruct
          org.opencontainers.image.vendor: AMD
          org.opencontainers.image.version: ""
        tag: 0.8.4
  - name: aim-meta-llama-llama-3-2-3b-instruct
    tags:
      - image: docker.io/amdenterpriseai/aim-meta-llama-llama-3-2-3b-instruct:0.8.4
        labels:
          com.amd.aim.description.full: ""
          com.amd.aim.hfToken.required: "True"
          com.amd.aim.model.canonicalName: meta-llama/Llama-3.2-3B-Instruct
          com.amd.aim.model.recommendedDeployments:
            "{'gpuModel': 'MI300X', 'gpuCount':
            1, 'precision': 'fp16', 'metric': 'latency', 'description': 'Optimized
            for latency on MI300X using fp16 precision'}, {'gpuModel': 'MI300X',
            'gpuCount': 1, 'precision': 'fp16', 'metric': 'throughput', 'description':
            'Optimized for throughput on MI300X using fp16 precision'}"
          com.amd.aim.model.source: https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct
          com.amd.aim.model.tags: text-generation, chat, instruction
          com.amd.aim.model.variants: meta-llama/Llama-3.2-3B-Instruct
          com.amd.aim.release.notes: ""
          com.amd.aim.title: meta-llama/Llama-3.2-3B-Instruct
          org.opencontainers.image.authors: ""
          org.opencontainers.image.created: "2025-11-11T10:45:11Z"
          org.opencontainers.image.description:
            Balanced instruction-tuned Llama 3.2 model
            with 3B parameters.
          org.opencontainers.image.documentation: ""
          org.opencontainers.image.licenses: llama3.2, MIT
          org.opencontainers.image.ref.name: ubuntu
          org.opencontainers.image.revision: bdba5c8c0a6be5962f098fa4bbe0ecbce9fdc07a
          org.opencontainers.image.source: https://github.com/silogen/aim-build
          org.opencontainers.image.title: meta-llama/Llama-3.2-3B-Instruct
          org.opencontainers.image.vendor: AMD
          org.opencontainers.image.version: ""
        tag: 0.8.4
