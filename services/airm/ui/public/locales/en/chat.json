{
  "title": "Chat",
  "modes": {
    "chat": "Chat",
    "compare": "Compare"
  },
  "chat": {
    "title": "Chat",
    "description": "Chat mode lets you experiment with individual models you're able to access. You can tweak various retrieval and generation settings to see how they impact the model's responses.",
    "tips": {
      "tip1": "Pick a model from the dropdown menu",
      "tip2": "Open settings to explore the options you can adjust."
    }
  },
  "compare": {
    "title": "Compare",
    "description": "Compare mode lets you experiment with models you're able to access in parallel. You can tweak various retrieval and generation settings to see how they impact the model's responses.",
    "tips": {
      "tip1": "Pick two models from the dropdown menus",
      "tip2": "Open settings to explore the options you can adjust."
    }
  },
  "roles": {
    "user": "You",
    "assistant": "Assistant",
    "system": "System"
  },
  "modelSettings": {
    "title": "Model settings",
    "selectModel": "Select model",
    "syncSettings": {
      "label": "Sync",
      "description": "Apply settings to both models"
    },
    "enableRag": {
      "label": "Enable RAG",
      "description": "Use this setting to toggle Retrieval Augmented Generation (RAG)."
    },
    "collection": {
      "label": "Collection",
      "description": "Select the collection you wish to use.",
      "tooltip": "You and your team can manage collections in the Collections tab."
    },
    "documentCount": {
      "label": "Document count",
      "description": "The number of documents to retrieve for each query.",
      "tooltip": "A too high document count may confuse the model and a too low count may not provide enough context to answer the query."
    },
    "hybridSearch": {
      "label": "Hybrid search",
      "description": "Toggle to perform a hybrid search.",
      "tooltip": "A hybrid search is a combination of semantic and keyword based search. Performing a hybrid search is mutually exclusive with setting a certainty threshold for the results."
    },
    "alpha": {
      "label": "Alpha",
      "description": "The balance of semantic search vs keyword search.",
      "tooltip": "An alpha value of 0 is a pure keyword search. An alpha value of 1 is a pure semantic search."
    },
    "certainty": {
      "label": "Certainty",
      "description": "Semantic similarity threshold for documents.",
      "tooltip": "A threshold controlling the minimum semantic similarity of matched documents to the query. A too high threshold may exclude pertinent documents and a too low threshold may include irrelevant documents."
    },
    "userPromptTemplate": {
      "label": "User prompt template",
      "description": "Enter a template to format user input and sources for the model.",
      "placeholder": "Enter your prompt here...",
      "tooltip": "This field specifies how the sources retrieved from the collection are formatted, along with the user input, when being sent to the LLM. Use the variable {{USER_MESSAGE}} to signify the user input and {{SOURCES_STRING}} to signify the sources retrieved.",
      "validationErrorMessage": "Invalid input. {{USER_MESSAGE}} and {{SOURCES_STRING}} should always be included."
    },
    "temperature": {
      "label": "Temperature",
      "description": "Control the predictability of the model.",
      "tooltip": "Higher values are more creative, lower more predictable."
    },
    "frequencyPenalty": {
      "label": "Frequency penalty",
      "description": "Fine grained control to penalize tokens frequently generated by the model so far.",
      "tooltip": "Control how the model penalizes or rewards new tokens based on their frequency in the text so far. Values more than 0 encourage the model to generate new tokens, while values less than 0 encourage the model to repeat tokens."
    },
    "presencePenalty": {
      "label": "Presence penalty",
      "description": "Fine grained control to penalize tokens already generated by the model so far.",
      "tooltip": "Control how the model penalizes or rewards new tokens based on their presence in the text so far. Values more than 0 encourage the model to generate new tokens, while values less than 0 encourage the model to repeat tokens."
    },
    "systemPrompt": {
      "label": "System prompt",
      "placeholder": "Enter your prompt here...",
      "description": "Enter a System Prompt to guide the model's response.",
      "tooltip": "A system prompt is the initial message or text sent to the model, providing guidelines on how it should respond. The quality and conciseness of a system prompt can significantly alter the accuracy of models response."
    }
  },
  "chatInput": {
    "placeholder": "Type your message here...",
    "placeholderDisabled": "Select model(s) to chat",
    "regenerateResponse": "Regenerate response"
  },
  "actions": {
    "selectModel": "Select model",
    "clearAll": "Clear chat"
  },
  "errors": {
    "modelLoadingFailed": "Error loading models",
    "workloadLoadingFailed": "Error loading workloads",
    "chatResponseFailed": "Error fetching chat response. Please try again.",
    "noCanonicalName": "No canonical name found"
  },
  "debugInfoModal": {
    "title": "Debug Information",
    "subTitle": "View prompts and token usage associated with this message.",
    "ragDocumentsTitle": "What documents were retrieved as part of RAG?",
    "ragDocumentsDescription": "These are the documents that were retrieved as part of RAG, along with their retrieval scores.",
    "noSources": "No sources available",
    "promptsTitle": "What prompts were sent to the LLM?",
    "promptsDescription": "These are the prompts that were sent to the LLM in order to generate the response. The LLM might choose to disregard one or more prompts, based on its template.",
    "noPromptMessages": "No prompt messages available",
    "tokenUsageTitle": "What was the token usage for the completion?",
    "promptTokens": "Prompt tokens:",
    "completionTokens": "Completion tokens:",
    "totalTokens": "Total tokens:",
    "noTokenUsage": "No token usage information available"
  }
}
